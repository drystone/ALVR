# How ALVR works

This document details some technologies used by ALVR.

If you have any doubt about what is (or isn't) written in here you can contact @zarik5, preferably on Discord.

This document was updated on May 12th 2023 and refers to the master branch.

## Table of contents

* Architecture
  * The packaged application
  * Programming anguages
  * Source code organization
* Logging and error management
  * The event system
* Session and settings
  * Procedural generation of code and UI
* The dashboard
  * The user interface
  * Driver communication
  * Driver lifecycle
* The streaming pipeline: Overview
* Client-driver communication
  * Discovery
  * Streaming
* SteamVR driver
* Client and driver compositors
  * Foveated rendering
  * Color correction
* Video transcoding
  * Encoder
  * Decoder
* Audio
* Tracking and synchronization
* Other input and output
* Upcoming
  * Phase sync
  * Sliced encoding

## Architecture

### The packaged application

ALVR is made of two applications: the streamer and client. The streamer can be installed on Windows and Linux, while the client is installed on Android VR headsets. The client communicates with the driver through TCP or UDP sockets.

The client is a single unified APK, named `alvr_client_android.apk`. It is powered by OpenXR and it is comptible with Quest headsets, recent Pico headsets and HTC Focus 3 and XR Elite.  

The streamer is made of two main parts: the dashboard and the driver (also known as server). The driver is dynamically loaded by SteamVR. This is the file structure on Windows:

* `bin/win64/`
  * `driver_alvr_server.dll`: The main binary, responsible for client discovery and streaming. Loaded by SteamVR.
  * `driver_alvr_server.pdb`: Debugging symbols
  * `openvr_api.dll`: OpenVR SDK used for updating the chanperone.
  * `vcruntime140_1.dll`: Windows SDK used by C++ code in the driver.
* `ALVR Dashboad.exe`: Dashboard binary used to change settings, manage clients, monitor statistics and do installation actions. It can launch SteamVR.
* `driver.vrdrivermanifest`: Auxiliary config file used by the driver.

At runtime, some other files are created:

* `session.json`: This contains unified configuration data used by ALVR, such as settings and client records.
* `session_log.txt`: Main log file. Each line is a json structure and represents an event generated by the driver. This gets cleared each time a client connects.
* `crash_log.txt`: Auxiliary log file. Same as  `session_log.txt`, except only error logs are saved, and does not get cleared.

### Programming languages

ALVR is written in multiple languages: Rust, C, C++, HLSL, GLSL. The main language used in the codebase is Rust, which is used for the dashboard, networking, video decoding and audio code. C and C++ are used for graphics, video encoding and SteamVR integration. HLSL is used for graphics shaders on the Windows driver, GLSL is used on the Linux driver and the client. Moving forward, more code will be rewritten from C/C++ to Rust and HLSL code will be moved to GLSL or WGSL.

Rust is a system programming language focused on memory safety and ease of use. It is as performant as C++ but Rust code is less likely to be affected by runtime bugs. The prime feature Rust feature used by ALVR is enums, that correspond to tagged unions in C++. Rust's enum is a data type that stores different kinds of data, but only one type can be accessed at a time. For example the type `Result` can contain either an `Ok` value or an `Err` value but not both. Together with pattern matching, this is the foundation of error management in Rust applications.  

### Source code organization

ALVR code is hosted in a monorepo. This is an overview of the git tree:

* `.github/`: Contains scripts used by the GitHub CI.
* `alvr/`: Each subfolder is a Rust crate ("crate" means a code library or executable).
  * `audio/`: Utility crate hosting audio related code shared by client and driver.
  * `client_core/`: Platform agnostic code for the client. It is used as a Rust library for `alvr_client_openxr` and can also compiled to a C ABI shared library with a .h header for integration with other projects.
  * `client_mock/`: Client mock implemented as a thin wrapper around `alvr_client_core`.
  * `client_openxr/`: Client implementation using OpenXR, compiled to a APK binary.
  * `common/`: Some common code shared by other crates. It contains code for versioning, logging, struct primitives, and OpenXR paths.
  * `dashboard/`: The dashboard application.
  * `events/`: Utility crate hosting code related to events.
  * `filesystem/`: Utility crate hosting code for filesystem abstraction between Windows and Linux.
  * `packets/`: Utility crate containing packet definitions for communication between client, driver and dashboard.
  * `server/`: The driver shared library loaded by SteamVR.
  * `server_io/`: Common functionality shared by dashboard and driver, for interaction with the host system. This allows dashboard and driver to work indepently from each other.
  * `session/`: Utility crate related to session file and data management.
  * `sockets/`: Utility crate shared by client and driver with socket and protocol implementation.
  * `vrcompositor_wrapper/`: Small script used on Linux to correctly load the ALVR Vulkan layer by SteamVR.
  * `vulkan_layer/`: Vulkan WSI layer used on Linux to work around limitations of the OpenVR API on Linux. This is mostly patchwork and hopefully will be removed in the future.
  * `xtask/`: Utility CLI hosting a variety of scripts for environment setting, building, and packaging ALVR. Should be called with `cargo xtask`.
* `packaging/`: Linux shell scripts for packaging built on top of `xtask`.
* `patches/`: git patches for the Linux FFmpeg build.
* `resources/`: resources for the README.
* `wiki/`: Contains the souurce for the Github ALVR wiki. Changes are mirrored to the actual wiki once commited.
* `about.toml`: Controls what dependency licenses are allowed in the codebase, and helps with generating the licenses file in the packaged ALVR streamer.
* `Cargo.lock`: Contains versioning information about Rust dependencies used by ALVR.
* `Cargo.toml`: Defined the list of Rust crates contained in the repository, and hosts some other worskpace-level Rust configuration.

## Logging and error management

Logging is split into interface and implementation. The interface is defined in `alvr/common/src/logging.rs`, the implementations are defined in each binary crate as `logging_backend.rs`.

ALVR logging system is based on the crate [log](https://crates.io/crates/log). `log` is already very powerful on its own, since its macros can collect messages, file and line number of the invocation.

ALVR defines some structures, macros and functions to ease error management. The base type used for error management is `StrResult<T>` that is an alias for `Result<T, String>`. Read more about Rust's Result type [here](https://doc.rust-lang.org/std/result/).

There are many ways of logging in ALVR, each one for different usecases. To make use of them you should add `use alvr_common::prelude::*` at the top of the Rust source file.

* `error!()`, `warn!()`, `info!()`, `debug!()` (reexported macros from the `log` crate). Log is processed depending on the logging backend.
* `show_e()` and `show_w()` are used to log a string message, additionally showing a popup.
* `show_err()`, `show_warn()` work similarly to `show_e()` and `show_w()`, but they accept a `Result<>` and log only if the result is `Err()`.
* `fmt_e!()` adds tracing information to a message and produces a `Err()`, that can be returned.
* `err!()` and `enone!()` used respectively with .`.map_err()` and `.ok_or_else()`, to map a `Result` or `Option` to a `StrResult`, adding tracing information.
* Some other similarly named funtions and macros with similar functionality

### The event system

Events are messages used internally in the driver and sent to dashboard instances. Events are generated with `send_event()` and is implemented on top of the logging system.

This is the layout of `Event`, in JSON form

```json
{
  "timestamp": "<timestamp>",
  "event_type": {
    "id": "<EventType>",
    "content": { <depends on id> }
  }
}
```

Log is a special kind of event:

```json
{
  "timestamp": "<timestamp>",
  "event_type": {
    "id": "Log",
    "content": {
      "severity": "Error or Warn or Info or Debug",
      "content": "<the message>"
    }
  }
}
```

The driver logs events in JSON form to `session.json`, one per line.

Currently its use is limited, but eventually this will replace the current logging system, and logging will be built on top of the event system. The goal is to create a unified star-shaped network where each client and dashboard instance sends events to the server and the server broadcasts events to all other clients and dashboard instances. This should also unify the way the server communicates with clients and dashboards, making the dashboard just another client.

## Session and settings

ALVR uses a unified configuration file, that is `session.json`. It is generated the first time ALVR is launched. This file contains the following top-level fields:

* `"server_version"`: the current version of the streamer. It helps during a version upgrade.
* `"drivers_backup"`: temporary storage for SteamVR driver paths. Used by the dashboard.
* `"openvr_config"`: contains a list of settings that have been checked for a diff. It is used by C++ code inside the driver.
* `"client_connections"`: contains entries corresponding to known clients.
* `"session_settings"`: all ALVR settings, laid in a tree structure.

### Procedural generation of code and UI

ALVR lays out settings in a tree-like structure, in a way that the code itself can efficiently make use of. Settings can contain variants (in `session.json` are specified in PascalCase), which represent mutually exclusive options.

ALVR uses the macro `SettingsSchema` in the `settings-schema` crate to generate auxiliary code, ie a schema and the "default representation" of the settings. This is a crate created specifically for ALVR but can be used for other projects too.

The schema is made of nested `SchemaNode`s that contain metadata. Some of the metadata is specified directly inside inline attributes in structures and enums.

The "default representation" (the type names are generated by concatenating the structure/enum name with `Default`), are structures that can hold settings in a way no not lose information about unselected variants; enums are converted to structs and variants that hold a value are converted to fields. The main goal of this is to meet the user expectation of not losing nested configuration when changing some options. The default representation is exactly what is saved inside `session.json` in `"session_settings"`.

Info about the various types of schema nodes can be found [here](https://github.com/zarik5/settings-schema-rs).

The dashboard makes use of schema metadata and the default representation to generate the settings UI. The end result is that the settings UI layout closely matches the structures used internally in the code, and this helps understanding the inner workings of the code.

When upgrading ALVR, the session might have a slightly different layout, usually some settings might have been added/removed/moved/renamed. ALVR is able to handle this by doing an extrapolation process: it starts from the default session, and replace values taken from the old session file with the help of the settings schema.

## The dashboard

The dashboard is the main way of interacting with ALVR. Functionality is organized in tabs.

### The User Interface

These are the main components:

TODO: Add screenshots

* Sidebar: is used to select the tab for the main content page.
* Connections tab: used to trust clients or add them manually specifying the IP
* Statistics tab: shows graphs for latency and FPS and a summary page
* Settings tab: settings page split between `Presets` and `All Settings`. `All Settings` are procedurally generated from a schema. `Presets` are controls that modify other settings.
* Installation tab: utilities for installation: setting firewall rules, registering the driver, launching the setup wizard.
* Logs tab: shows logs and events in a table.
* Debug tab: debugging actions.
* About tab: information about ALVR.
* Lower sidebar button: can be either "Launch SteamVR" or "Restart SteamVR", depending on the driver connection status
* Notification bar: shows log in a non-obstructive way.

### Driver communication

The dashboard communcates with the driver in order to update its information and save configuration. This is done thorugh a HTTP API. These are the endpoints:

* `/api/dashboard-request`: This is the main URL used by the dashboard to send messages and data to the server. The body contains the specific type and body of the request.
* `/api/events`: This endpoint is upgraded to a websocket and is used for listening to events from the driver
* `/api/ping`: returns code 200 when the driver is alive.

The dashboard retains some functionality when the driver is not launched. It can manage settings, clients and perform installation actions, but clients cannot be discovered. Once The driver is launched all these actions are performed by the server, requested with the HTTP API. This mechanism ensures that there are no data races.

### Driver lifecycle

The dashboard is able to launch and restart SteamVR, in order to manage the driver's lifecycle.

The driver launch procedure is as follows:

* The driver is registered according to the "Driver launch action" setting, if needed. By default, current SteamVR drivers are unregistered and backed up inside `session.json`.
* On Linux, the vrcomposiotr wrapper is installed if needed
* SteamVR is launched.

Once the drivers shuts down, if there are backed up drivers, these are restored.

The driver restart procedure is as follows:

* The dashboard notifies the driver that it should be restarted.
* The driver sends a request for restart to the dashboard.
* The driver asks SteamVR to shutdown, never unregistering drivers.
* The dashboard waits for SteamVR to shutdown, otherwise killing it after a timeout.
* The dashboard relaunches SteamVR.

This might seem unnecessarily complicated. The reason for the first message round trip is to plug-in to the existing restarting system used by settings invalidation, which is invoked from the driver itself. The reason which the driver cannot be autonomous in restarting is because any auxiliary process spawned by the driver will block SteamVR shutdown or leave it in a zombie state.

## The streaming pipeline: Overview

The goal of ALVR is to bridge input and output of a PCVR application to a remote headset. In order to do this ALVR implements pipelines to handle input, video and audio. The tracking-video pipeline (as known as the motion-to-photon pipeline) is the most complex one and it can be summarized in the following steps:

* Poll tracking data on the client
* Send tracking to the driver
* Execute the PCVR game logic and render layers
* Compose layers into a frame
* Encode the video frame
* Send the encoded video frame to the client through the network
* Decode the video frame on the client
* Perform more compositor transformations
* Submit the frame to the VR runtime
* The runtime renders the frame during a vsync.

## Client-driver communication

ALVR uses a custom protocol for client-driver communication. ALVR supports UDP and TCP transports. USB connection is supported although not as a first class feature; you can read more about it [here](https://github.com/alvr-org/ALVR/wiki/ALVR-wired-setup-(ALVR-over-USB)).

### Discovery

Usually the first step to establish a connection is discovery. When the server discovers a client it shows it in the "New clients" section in the Connection tab. The user can then trust the client and the connection is established.

ALVR uses a UDP socket at 9943 for discovery. The client broadcasts a packet and waits for the driver to respond. It's the client that broadcasts and it's the driver that then asks for a connection: this is because of the balance in responsibility of the two peers. The client becomes the portal though a PC, that can contain sensitive data. For this reason the server has to trust the client before initiating the connection.

This is the layout of the discovery packet

|      Prefix       | Protocol ID | Hostname |
| :---------------: | :---------: | :------: |
| "ALVR" + 0x0 x 12 |   8 bytes   | 32 bytes |

* The prefix is used to filter packets and ensure a packet is really sent by an ALVR client
* The protocol ID is a unique version identifier calculated from the semver version of the client. If the client version is *semver-compatible* with the streamer, the protocol ID will match.
* Hostname: the hostname is a unique identifier for a client. When a client is 

The format of the packet can change between major versions, but the prefix must remain unchanged, and the protocol ID must be 8 bytes.

### Streaming

ALVR uses two sockets for streaming: the control socket and stream socket. Currently these are implemented with async code; thete's a plan to move this back to sync code.

The control socket uses the TCP trasport; it is used to exchange small messages between client and server, ALVR requeres TCP to ensure reliablity.

The stream socket can use UDP or TCP; it is used to send large packets and/or packets that do not require reliability, ALVR is robust to packet losses and packet reordering.

The specific packet format used over the network is not clearly defined since ALVR uses multiple abstraction layers to manipuate the data (bincode, tokio lenght delimited coding). Furthermore, packets are broken up into shards to ensure they can support the MTU when using UDP.

Since the amount of data streamed is large, the socket buffer size is increased both on the driver side and on the client.

<!-- ## Event timing

`EventTiming` is a general purpose mathematical tool used to manage timing for cyclical processes. Some "enqueue" and "dequeue" events are registered and `EventTiming` outputs some timing hints to minimize the queuing time for the next events.

Currently, `EventTiming` is used for the stream socket throttling buffer and audio implementations, but it will be also used for video frame timing (to reduce latency and jitter), total video latency estimation (to reduce the black pull and positional lag), controller timing and maybe also controller jitter.

`EventTiming` supports two operation modes: fixed latency and automatic latency.

### Fixed latency mode

In fixed latency mode, `EventTiming` calculates the average latency between corresponding enqueue and dequeue events.

Todo

### Automatic latency mode

Todo -->

Updated content ends here

---------------------------
WIP:

## SteamVR driver

The driver is the component responsible for most of the streamer functionality. It is implemented as a shared library loaded by SteamVR. It implements the [OpenVR API](https://github.com/ValveSoftware/openvr) in order to interface with SteamVR.

The OpenVR API is used to push tracking and button data to SteamVR and obtain game frames obtained   While ALVR implements the `IVRDriverDirectModeComponent` interface



## Foveated encoding

Foveated encoding is a technique where frame images are individually compressed in a way that the human eye barely detects the compression. Particularly, the center of the image is kept at original resolution, and the rest is compressed. In practice, first the frames are re-rendered on the server with the outskirts of the frame "squished". The image is then transmitted to the client and then it gets re-expanded by using an inverse procedure.

But why does this work? The human eye has increased acuity in the center of the field of vision (the fovea) with respect to the periphery.

Foveated encoding should not be confused with foveated rendering, where the image is rendered to begin with at a lower resolution in certain spots. Foveated encoding will NOT lower your GPU usage, only the network usage.

Currently ALVR does not directly support foveated encoding in the strict sense, instead it uses *fixed* foveated encoding. In a traditional foveated encoding application, the eyes are tracked, so that only what is directly looked at is rendered at higher resolution. But currently none of the headset supported by ALVR support eye tracking. For this reason, ALVR does foveated encoding by pretending the user is looking straight at the center of the image, which most of time is true.

Here are explained three foveated encoding algorithms.

### Warp

Developed by @zarik5. This algorithm applies an image compression that most adapts to the actual acuity graph of the human eye. It compresses the image radially (with an ellipse as the base) from a chosen spot in the image, with a chosen monotonic function. This algorithm makes heavy use of derivatives and inverse functions. It is implemented using a chain of shaders (shaders are a small piece of code that is run on the GPU for performance reasons). You can explore an interactive demo at [this link](https://www.shadertoy.com/view/3l2GRR).

This algorithm is actually NOT used by ALVR. It used to be, but it got replaced by the "slices" method. The warp method has a fatal flaw: the pixel alignment is not respected. This causes resampling that makes the image look blurry.

### Slices

Developed by @zarik5. This is the current algorithm used by ALVR for foveated encoding. The frame is cut into 9 rectangles (with 2 vertical and 2 horizontal cuts). Each rectangle is rendered at a different compression level. The center rectangle is uncompressed, the top/bottom/left/right rectangle is compressed 2x, the corner rectangles are compressed 4x. These cuts are actually virtual (mathematical) cuts, that are executed all at once in a single shader pass. All slices are neatly packed to form a new rectangular image. You can explore an interactive demo at [this link](https://www.shadertoy.com/view/WddGz8).

This algorithm is much simpler than the warp method but it is still quite complex. The implementation takes into account pixel alignment and uses some margins in the rectangles to avoid color bleeding. Like the warp algorithm, the slices method was designed to support eye tracking support when it will be available in consumer hardware.

### Axis-Aligned Distorted Transfer (AADT)

This algorithm was developed by Oculus for the Oculus Link implementation. It is simpler than the other two methods, the end result looks better but it has less compression power. Like the slices algorithm, the image is cut into 9 rectangles where each rectangle is compressed independently. But actually the top and bottom rectangles are compressed only vertically, and the left and right only horizontally. This type of compression lends itself well to be used for images rendered in VR headsets, since it works in the same direction (and not against) the image distortion needed for lens distortion correction.

It is planned to replace the slices method with AADT in the future.

## Audio

Todo
